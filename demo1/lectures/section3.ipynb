{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仮説設定をしてKPIを定めてみよう\n",
    "\n",
    "データとして出てくるのが以下のようなデータです。\n",
    "\n",
    "- check：決済した\n",
    "- id: ユーザID\n",
    "- money:いくら使った\n",
    "\n",
    "```\n",
    "+-------+---+-----+\n",
    "|actions| id|money|\n",
    "+-------+---+-----+\n",
    "|  check|  2|  200|\n",
    "|  check|  1| 1000|\n",
    "+-------+---+-----+\n",
    "\n",
    "```\n",
    "\n",
    "例えば、\n",
    "現状は、お金を使ってくれるユーザの売上（実際だと直近1週間などで計算することが多い）が5000円以上のユーザをロイヤルユーザとして  \n",
    "そこに分類する人たちがより多くのおかねを使ってくれるのではないか？といった施作が考えられる。\n",
    "\n",
    "そこで、お金をたくさん使ってくれるユーザ(ロイヤルユーザ)に対して、Aの広告を出力する  \n",
    "お金をあまり使ってくれないユーザに対して、Bの広告を出力する\n",
    "クーポンを配布するなどでもOK。\n",
    "\n",
    "今回は、簡単のために\n",
    "モデルの成功としては、現状の全体の売上1200円(KPI)でそれが１０％上がれば成功といえる(ことにする)。\n",
    "（経費は分かりやすさのため0円）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回の環境についての説明 \n",
    "環境説明のためのコマンド群\n",
    "\n",
    "## 環境の立ち上げ\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "## dockerコンテナへログイン\n",
    "\n",
    "```\n",
    "docker exec -it pyspark_mlops /bin/bash\n",
    "```\n",
    "\n",
    "## pyspark-topicの作成コマンド\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka_2.13-3.0.2/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka_mlops:9092 \n",
    "```\n",
    "\n",
    "## Pyspakrでストリーミングデータを読み取る\n",
    "\n",
    "### 接続のためのコマンド\n",
    "\n",
    "```\n",
    "pyspark --packages org.apache.spark:spark-streaming_2.13:3.2.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.4,org.apache.spark:spark-avro_2.12:3.2.4\n",
    "```\n",
    "\n",
    "### ストリーミングの準備\n",
    "\n",
    "```\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka_mlops:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic\") \\\n",
    "  .load()\n",
    "```\n",
    "\n",
    "### ストリーミングデータの読み込み\n",
    "\n",
    "```\n",
    "file_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/tmp/share_file/datalake/web_actions/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .partitionBy(\"key\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/parquet/\") \\\n",
    "  .start()\n",
    "```\n",
    "\n",
    "### 停止するときはこちら\n",
    "\n",
    "```\n",
    "file_stream.stop()\n",
    "```\n",
    "\n",
    "### データの確認を行う\n",
    "http://localhost:3001/done/?id=1\n",
    "\n",
    "df = spark.read.parquet(\"/tmp/share_file/datalake/web_actions/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの取得と蓄積\n",
    "末尾のIDやURLを変えながら、何回かデータを送ってみます。  \n",
    "IDは最低でも3人以上用意してみましょう。\n",
    "\n",
    "### 買い上げ完了画面\n",
    "http://localhost:3001/done/?id=1\n",
    "\n",
    "## データの読み込みと書き込みでデータウェアハウスにデータを保存してきましょう\n",
    "\n",
    "```\n",
    "df=spark.read.parquet(\"/tmp/share_file/datalake/web_actions\")\n",
    "```\n",
    "\n",
    "### unable to infer schemaが出てしまった場合\n",
    "\n",
    "以下のようにスキーマ設定をしてみてください。\n",
    "\n",
    "```\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"value\", StringType(), False),\n",
    "    StructField(\"key\", StringType(), False)\n",
    "])\n",
    "\n",
    "df=spark.read.parquet(\"/tmp/share_file/datalake/web_actions\", inferSchema=False, schema=struct)\n",
    "\n",
    "```\n",
    "\n",
    "### jsonをバラバラにして扱いやすくする\n",
    "\n",
    "```\n",
    "df.createOrReplaceTempView(\"web_actions\")\n",
    "\n",
    "result_df=spark.sql(\"select key,id,money,action,sendtime from web_actions LATERAL VIEW json_tuple(value,'id','money','action','sendtime') user as id, money, action, sendtime\")\n",
    "```\n",
    "\n",
    "### ファイルをparquetで吐き出す\n",
    "\n",
    "```\n",
    "result_df.coalesce(1).write.mode('overwrite').parquet(\"/tmp/share_file/datamart/web_actions/\")\n",
    "```\n",
    "\n",
    "# データを読み込んでみる\n",
    "\n",
    "```\n",
    "df2 = spark.read.parquet(\"/tmp/share_file/datamart/web_actions/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMLで簡単なモデルを作成してみましょう\n",
    "\n",
    "今回はKmeansを使って、データを2つに分類していきます。\n",
    "\n",
    "分類に利用するデータは、売上で売上が高い人と低い人で分類を行なっていきます。\n",
    "\n",
    "lectureフォルダに配置された「KMeans.py」をみながら話を進めていきましょう。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの結果をデータベースに保存してみよう\n",
    "\n",
    "いよいよモデルの結果をデプロイしていきます。  \n",
    "今回はモデルの結果をmongodbに格納してNodeJs上のアプリケーションから利用していきます。\n",
    "\n",
    "## mongodbへの保存を行なっていきます\n",
    "\n",
    "```\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 \\\n",
    "        --conf spark.mongodb.input.uri=mongodb://action:pass123@mongo_data_mlops:27017/user_prediction \\\n",
    "        --conf spark.mongodb.output.uri=mongodb://action:pass123@mongo_data_mlops:27017/user_prediction\n",
    "```\n",
    "\n",
    "## parquetの読み込み\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"/tmp/share_file/datamodel/part2\")\n",
    "```\n",
    "\n",
    "## mongodbへの書き込みを行う\n",
    "\n",
    "```\n",
    "df.repartition(1).write \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option( \"uri\", \"mongodb://action:pass123@mongo_data_mlops:27017/user_prediction.prediction\") \\\n",
    "    .save()\n",
    "```\n",
    "\n",
    "## mongodbのデータを確認してみよう\n",
    "書き込んだデータを確認してみましょう。\n",
    "\n",
    "### 対象のコンテナに接続\n",
    "\n",
    "```\n",
    "docker exec -it mongo_data_mlops /bin/bash\n",
    "```\n",
    "\n",
    "### mongodbに接続を行う\n",
    "mongo -u action -p pass123 user_prediction\n",
    "\n",
    "### 簡単にいくつか検索してみます\n",
    "predictionはテーブル名\n",
    "\n",
    "```\n",
    "db.prediction.find()\n",
    "db.prediction.find({id:1})\n",
    "db.prediction.find({id:1},{prediction:1, _id:0})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
